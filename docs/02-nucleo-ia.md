# üß† N√∫cleo de Intelig√™ncia Artificial (Luma)

A Luma n√£o √© "m√°gica", √© pura engenharia de prompt e gerenciamento de estado.

## üèóÔ∏è Engenharia de Prompt

O prompt enviado ao Gemini n√£o √© apenas o que o usu√°rio escreveu. √â um "sandu√≠che" de informa√ß√µes montado em `LumaHandler.js`:

### Estrutura do Prompt (System Instruction)

```text
[IDENTIDADE]
Nome: Luma.
Contexto: Voc√™ √© uma assistente sarc√°stica... (Vem de lumaConfig.js)

[ESTILO]
Use g√≠rias, seja breve, n√£o use linguagem rob√≥tica.

[TRA√áOS OBRIGAT√ìRIOS]
- Zoa o usu√°rio se a pergunta for √≥bvia.
- Responde em 1 ou 2 frases.

[FORMATO WHATSAPP]
1. SEJA BREVE.
2. ECONOMIA DE PALAVRAS.

[HIST√ìRICO]
Usu√°rio: Oi
Luma: Fala logo.
(√öltimas 20 mensagens)

[USU√ÅRIO ATUAL]
{{mensagem_do_usuario}}
```

### Anatomia de um Prompt Real

```javascript
// src/handlers/LumaHandler.js (Simplificado)
class LumaHandler {
    buildPrompt(userMessage, chatId) {
        const personality = this.getPersonality(chatId);
        const history = this.getHistory(chatId);
        
        const systemInstruction = `
${personality.identity}

${personality.style}

REGRAS OBRIGAT√ìRIAS:
${personality.rules.join('\n')}

HIST√ìRICO DA CONVERSA:
${this.formatHistory(history)}
        `.trim();
        
        return {
            systemInstruction,
            userMessage
        };
    }
    
    formatHistory(history) {
        return history
            .slice(-20) // √öltimas 20 mensagens
            .map(msg => `${msg.role}: ${msg.content}`)
            .join('\n');
    }
}
```

## üé≠ Sistema de Personalidades

As personalidades n√£o s√£o apenas "tons diferentes". S√£o instru√ß√µes estruturadas que moldam comportamento.

### Personalidade: Padr√£o (default)

```javascript
// config/lumaConfig.js
personalities: {
    default: {
        identity: `
            Voc√™ √© Luma, uma IA assistente criada para WhatsApp.
            Sua personalidade √© amig√°vel, mas direta.
            Voc√™ n√£o gosta de perder tempo com enrola√ß√£o.
        `,
        style: `
            - Use g√≠rias brasileiras naturalmente (ex: "mano", "tipo")
            - Seja concisa. Prefira 1-2 frases.
            - N√£o use emojis em excesso (m√°x 1 por mensagem)
            - Evite linguagem formal ou corporativa
        `,
        rules: [
            'Nunca finja que tem emo√ß√µes reais',
            'Admita quando n√£o souber algo',
            'Zoe gentilmente perguntas √≥bvias',
            'Seja √∫til, mas n√£o bajuladora'
        ],
        examples: [
            {
                user: 'Oi',
                luma: 'E a√≠? Precisando de algo ou s√≥ t√° testando se funciono?'
            },
            {
                user: 'Qual a capital do Brasil?',
                luma: 'Bras√≠lia. S√©rio que voc√™ n√£o sabia isso?'
            }
        ]
    }
}
```

### Personalidade: Agressiva

```javascript
aggressive: {
    identity: `
        Voc√™ √© Luma no modo "sincerona".
        Sem papas na l√≠ngua. Voc√™ fala o que pensa.
    `,
    style: `
        - Use sarcasmo pesado
        - Seja mais direta e menos educada
        - Pode xingar de leve (sem palavr√µes pesados)
    `,
    rules: [
        'Zoe TODAS as perguntas bobas',
        'N√£o tenha medo de ser rude',
        'Mas ainda seja √∫til quando necess√°rio'
    ]
}
```

### Como Trocar Personalidade por Grupo

```javascript
// No chat do WhatsApp:
Usuario: !setpersona aggressive

// O que acontece:
MessageHandler ‚Üí CommandRouter ‚Üí DatabaseService.setPersonality(groupJID, 'aggressive')

// Pr√≥xima mensagem:
LumaHandler.getPersonality(groupJID) // Retorna 'aggressive'
```

## üñºÔ∏è Vis√£o Computacional (Multimodalidade)

Quando o usu√°rio envia uma imagem, o fluxo muda:

### Pipeline de Imagem + Texto

```javascript
async generateResponseWithImage(imageBuffer, text, chatId) {
    // 1. Converte imagem para Base64
    const base64Image = imageBuffer.toString('base64');
    
    // 2. Prepara objeto multimodal
    const parts = [
        {
            inlineData: {
                mimeType: 'image/jpeg',
                data: base64Image
            }
        },
        {
            text: text || 'O que voc√™ v√™ nesta imagem?'
        }
    ];
    
    // 3. Usa modelo com suporte a vis√£o
    const model = genAI.getGenerativeModel({
        model: 'gemini-2.0-flash-exp',
        systemInstruction: this.buildVisionPrompt()
    });
    
    // 4. Gera resposta
    const result = await model.generateContent(parts);
    return result.response.text();
}

buildVisionPrompt() {
    return `
        Voc√™ est√° analisando uma imagem enviada no WhatsApp.
        Responda como se estivesse vendo a imagem em tempo real.
        
        REGRAS:
        - Seja descritiva mas concisa (2-3 frases)
        - Se for um meme, reaja com humor
        - Se for uma pergunta visual (ex: "quanto √© isso?"), responda objetivamente
        - Mantenha sua personalidade sarc√°stica
    `;
}
```

### Modelos Dispon√≠veis e Capacidades

| Modelo | Vis√£o | Velocidade | Custo | Quando usar |
|--------|-------|------------|-------|-------------|
| `gemini-2.0-flash-exp` | ‚úÖ | Muito R√°pida | Gr√°tis | Produ√ß√£o (experimental) |
| `gemini-1.5-flash` | ‚úÖ | R√°pida | Gr√°tis | Fallback est√°vel |
| `gemini-1.5-flash-8b` | ‚ùå | Ultra R√°pida | Gr√°tis | Texto puro, alta carga |
| `gemini-1.5-pro` | ‚úÖ | Lenta | Pago | An√°lises complexas |

## üíæ Gerenciamento de Mem√≥ria (Contexto)

O Gemini API n√£o tem mem√≥ria ("Stateless"). N√≥s precisamos enviar o hist√≥rico a cada mensagem.

### Estrutura em Mem√≥ria

```javascript
class LumaHandler {
    constructor() {
        // Map<chatId, Array<Message>>
        this.conversationHistory = new Map();
        
        // Configura√ß√µes
        this.maxHistoryLength = 20;
        this.maxHistoryAge = 2 * 60 * 60 * 1000; // 2 horas
    }
    
    addToHistory(chatId, role, content) {
        if (!this.conversationHistory.has(chatId)) {
            this.conversationHistory.set(chatId, []);
        }
        
        const history = this.conversationHistory.get(chatId);
        
        history.push({
            role,      // 'user' ou 'model'
            content,
            timestamp: Date.now()
        });
        
        // Mant√©m apenas as √∫ltimas N mensagens
        if (history.length > this.maxHistoryLength) {
            history.shift(); // Remove a mais antiga
        }
    }
    
    getHistory(chatId) {
        const history = this.conversationHistory.get(chatId) || [];
        
        // Remove mensagens muito antigas
        const cutoff = Date.now() - this.maxHistoryAge;
        return history.filter(msg => msg.timestamp > cutoff);
    }
}
```

### Limpeza Autom√°tica (Garbage Collection)

```javascript
// src/managers/MemoryManager.js
class MemoryManager {
    startCleanupTask() {
        // Roda a cada 1 hora
        setInterval(() => {
            this.cleanupStaleHistories();
        }, 60 * 60 * 1000);
    }
    
    cleanupStaleHistories() {
        const cutoff = Date.now() - (2 * 60 * 60 * 1000);
        let cleaned = 0;
        
        for (const [chatId, history] of LumaHandler.conversationHistory) {
            const validMessages = history.filter(
                msg => msg.timestamp > cutoff
            );
            
            if (validMessages.length === 0) {
                LumaHandler.conversationHistory.delete(chatId);
                cleaned++;
            } else {
                LumaHandler.conversationHistory.set(chatId, validMessages);
            }
        }
        
        console.log(`[MemoryManager] Limpou ${cleaned} conversas inativas`);
    }
}
```

### Por que N√£o Salvamos Tudo no Banco?

**Vantagens de RAM:**
- ‚ö° Acesso instant√¢neo (ns vs ms)
- üîÑ N√£o precisa parsear JSON
- üßπ Limpeza autom√°tica

**Desvantagens:**
- üíæ Perde hist√≥rico ao reiniciar
- üìä N√£o √© poss√≠vel analisar conversas antigas

**Solu√ß√£o H√≠brida (Opcional):**
```javascript
// Salva apenas m√©tricas, n√£o conte√∫do completo
DatabaseService.logConversation({
    chatId,
    messageCount: history.length,
    lastActivity: Date.now(),
    // N√ÉO salva o texto por privacidade
});
```

## üîÑ Rota√ß√£o de Modelos (Fallback System)

Para garantir alta disponibilidade, implementamos um sistema de tentativas:

```javascript
async generateResponse(text, chatId) {
    const models = [
        'gemini-2.0-flash-exp',    // Tentativa 1: Mais inteligente
        'gemini-1.5-flash',        // Tentativa 2: Est√°vel
        'gemini-1.5-flash-8b'      // Tentativa 3: Leve
    ];
    
    for (let i = 0; i < models.length; i++) {
        try {
            const result = await this.callGemini(models[i], text, chatId);
            
            // Sucesso - registra qual modelo funcionou
            DatabaseService.incrementMetric(`model_${models[i]}_success`);
            return result;
            
        } catch (error) {
            console.log(`Modelo ${models[i]} falhou: ${error.message}`);
            
            // Se for rate limit ou erro tempor√°rio, tenta pr√≥ximo
            if (this.isRetryableError(error) && i < models.length - 1) {
                console.log(`Tentando modelo ${models[i + 1]}...`);
                continue;
            }
            
            // Se for erro fatal, n√£o tenta pr√≥ximo
            throw error;
        }
    }
    
    throw new Error('Todos os modelos falharam');
}

isRetryableError(error) {
    const retryableCodes = [
        429, // Too Many Requests
        503, // Service Unavailable
        500  // Internal Server Error
    ];
    return retryableCodes.includes(error.statusCode);
}
```

### Logs de Tentativas

```
[LumaHandler] Tentando gemini-2.0-flash-exp...
[LumaHandler] ‚úó Erro 429: Rate limit exceeded
[LumaHandler] Tentando gemini-1.5-flash...
[LumaHandler] ‚úì Resposta gerada em 1.2s
```

## üéØ Otimiza√ß√µes de Prompt

### T√©cnica 1: Few-Shot Learning

Inclu√≠mos exemplos de conversas no prompt para melhorar a qualidade:

```javascript
examples: [
    {
        user: 'Como fa√ßo bolo?',
        luma: 'Tem mil receitas na internet, mano. Especifica: chocolate? Cenoura? S√≥ misturar farinha com ovo n√£o d√° certo.'
    },
    {
        user: 'Me ajuda com matem√°tica',
        luma: 'Manda o problema. Mas se for conta b√°sica, usa a calculadora do celular.'
    }
]
```

Esses exemplos "ensinam" o modelo a responder no estilo da Luma.

### T√©cnica 2: Controle de Tokens

```javascript
const generationConfig = {
    maxOutputTokens: 150,      // Limita tamanho da resposta
    temperature: 0.9,          // Criatividade (0-2)
    topP: 0.8,                 // Diversidade de vocabul√°rio
    topK: 40                   // N√∫mero de palavras consideradas
};
```

**Explica√ß√£o:**
- `temperature` alta ‚Üí Respostas mais variadas e criativas
- `temperature` baixa ‚Üí Respostas mais previs√≠veis e factuais
- `maxOutputTokens` ‚Üí For√ßa a IA a ser concisa (ideal para WhatsApp)

### T√©cnica 3: Safety Settings

```javascript
const safetySettings = [
    {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_NONE' // Permite linguagem informal
    },
    {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM' // Bloqueia apenas casos graves
    }
];
```

Ajustamos para permitir g√≠rias e informalidade sem bloquear a IA.

## üìä Monitoramento de Performance

```javascript
async generateResponse(text, chatId) {
    const startTime = Date.now();
    
    try {
        const response = await this.callGemini(text, chatId);
        const duration = Date.now() - startTime;
        
        // Registra tempo de resposta
        DatabaseService.logMetric('ai_response_time', duration);
        
        // Alerta se estiver lento
        if (duration > 5000) {
            console.warn(`[LumaHandler] Resposta demorou ${duration}ms`);
        }
        
        return response;
    } catch (error) {
        DatabaseService.incrementMetric('ai_errors');
        throw error;
    }
}
```

## üß™ Testando a IA Localmente

```javascript
// test/luma-test.js
const LumaHandler = require('../src/handlers/LumaHandler');

async function testLuma() {
    const luma = new LumaHandler();
    
    // Simula conversa
    const chatId = 'test-chat';
    
    const response1 = await luma.generateResponse('Oi', chatId);
    console.log('Luma:', response1);
    
    const response2 = await luma.generateResponse('Me explica IA', chatId);
    console.log('Luma:', response2);
    
    // Verifica se hist√≥rico foi salvo
    const history = luma.getHistory(chatId);
    console.log('Hist√≥rico:', history);
}

testLuma();
```

---

**Pr√≥ximo passo**: Aprenda sobre processamento de m√≠dia em [03-motor-midia.md](./03-motor-midia.md)